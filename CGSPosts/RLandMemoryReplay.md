---
layout: page
title: Cognitive Science
mathjax: true
permalink: /cogsci/RL-and-Memory-Replay/
---

---
## Memory Replay in Biological and Deep Reinforcement Learning: A Comparison
A tried and tested source of inspiration for artificial intelligence has always been biology, and neural networks are no different. One of the most powerful approaches, reinforcement learning, is directly inspired by the natural process through which animals learn, namely a reward-and-punishment system paired with a trial-and-error mechanism of action. Combined with deep neural networks, this approach has seen unprecedented success.

Considering the success of the biological process $\rightarrow$ computational algorithm approach, the natural next step is to draw as much from biology as we can; yet, this brings to the forefront our own shortcomings in understanding ourselves. Neuroscience and its ability to explain the details of complex cognitive processes (in consistent and demonstrable terms) is notoriously deficient - it remains a fact that we simply lack fully coherent explanations of many cognitive tasks. 

However, if we can achieve groundbreaking results by implementing biological methods of learning into computational neural networks, could it possibly teach us a few things about how our own cognitive processes work as well?

There is an interesting article I came across, titled [*Learning Offline: Memory Replay in Biological
and Artificial Reinforcement Learning*](https://arxiv.org/pdf/2109.10034.pdf), by Roscow et. al, which explores the connections between the two types of reinforcement learning, and specifically how the concept of experience replay is involved.

The authors elaborate how it is related to the success of these modern-day deep reinforcement learning algorithms:

>  Key to the success of these algorithms is the practice of interleaving new trials with old ones, a technique known as experience replay.

___

<div class="date">
    Written on September 24, 2021
  </div>
